{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "from models import GeneratorForMnistGLO as Gen\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle\n",
    "def save_pickle(path,object):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(object, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot  as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IMAGES = 0\n",
    "CONTENT_LABEL = 2\n",
    "CLASS_LABEL = 1\n",
    "LR=\"ADAM_DEFULAT\"\n",
    "EPOCHS=300\n",
    "noise_std=5\n",
    "BATCH_SIZE=32\n",
    "WD=0\n",
    "CLASS_DIM = 5\n",
    "CONTENT_DIM = 25\n",
    "TOTAL_VEC_DIM = CLASS_DIM+CONTENT_DIM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "ToDo\n",
    "- Each style : Thick , rotated , thin\n",
    "- Does an overfit model can detect style?\n",
    "- Flow chart of distances to numbers.\n",
    "- Maybe clusters\n",
    "- Comapre weight decay  and L1,L2 Loss\n",
    "- Compare differnet wieght decay values : 0 , 0.01,0.1,0.5 and show the results.\n",
    "- THAT'S IT , NO MORE YOU HAVE TO MOVE ON.\n",
    "\n",
    "- If time allow : compare different dimensions for vector dim , compare different noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 27405.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "mnist_d=datasets.MNIST(2000)\n",
    "digits=np.zeros((2000,28,28))\n",
    "for i,data in enumerate(mnist_d):\n",
    "    digits[i,:,:]=data[0]\n",
    "digits=torch.tensor(digits)\n",
    "print(digits.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GLO(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, code_dim, out_channels=1):\n",
    "        super(GLO, self).__init__()\n",
    "        self.embedding_content=nn.Embedding(2000,CONTENT_DIM)\n",
    "        self.embedding_class=nn.Embedding(10,CLASS_DIM)\n",
    "        self.generator=Gen(code_dim,out_channels=out_channels)\n",
    "\n",
    "    def forward(self, class_id,image_id):\n",
    "        curr_class_embedding=self.embedding_class(class_id)\n",
    "        curr_content_embedding=self.embedding_content(image_id)\n",
    "        if self.training:\n",
    "            curr_content_embedding+=torch.normal(0.0,noise_std,size=curr_content_embedding.shape)\n",
    "            \n",
    "        combined=torch.cat((curr_content_embedding,curr_class_embedding),dim=1)\n",
    "        generated_pic=self.generator(combined)\n",
    "        generated_pic=torch.squeeze(generated_pic,dim=1)\n",
    "        return generated_pic\n",
    "\n",
    "    def get_emb_class(self,num):\n",
    "        return self.embedding_class(num)\n",
    "    def get_emb_content(self,id):\n",
    "        return self.embedding_content(id)\n",
    "\n",
    "    def save_weights(self,name):\n",
    "        return save_pickle(name,self.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "cur_model=GLO(TOTAL_VEC_DIM)\n",
    "print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# image=np.zeros((2000,1,28,28))\n",
    "# ids=np.zeros(2000)\n",
    "# classes=np.zeros(2000)\n",
    "# a=np.arange(10)\n",
    "# import torchvision.models as models\n",
    "\n",
    "# IMG_SIZE=224\n",
    "# num_pics=np.zeros((10,1,28,28))\n",
    "# for i,pic in enumerate(mnist_d):\n",
    "#     ids[i]=pic[2]\n",
    "#     curr_num = pic[1]\n",
    "#     classes[i]=curr_num\n",
    "#     image[i,:,:,:]=pic[0]\n",
    "\n",
    "#     if curr_num in a:\n",
    "#         num_pics[curr_num,:,:,:]=pic[0]\n",
    "#         a[curr_num]=-1\n",
    "        \n",
    "# from collections import namedtuple\n",
    "# import torchvision.models as models\n",
    "\n",
    "# import cv2\n",
    "\n",
    "# def resize(img_array):\n",
    "#     tmp = np.empty((img_array.shape[0],3, IMG_SIZE, IMG_SIZE))\n",
    "#     for i in range(len(img_array)):\n",
    "#         img = img_array[i].reshape(28, 28).astype('uint8')\n",
    "#         img = cv2.resize(img_array[i], (IMG_SIZE, IMG_SIZE))\n",
    "#         img = img.astype('float32')/255\n",
    "#         tmp[i,0,:,:] = img\n",
    "#         tmp[i,1,:,:] = img\n",
    "#         tmp[i,2,:,:] = img\n",
    "\n",
    "        \n",
    "#     return tmp\n",
    "\n",
    "LossOutput = namedtuple(\"LossOutput\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\"])\n",
    "# https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3\n",
    "class LossNetwork(torch.nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        super(LossNetwork, self).__init__()\n",
    "        self.vgg_layers = vgg_model.features\n",
    "        self.layer_name_mapping = {\n",
    "            '3': \"relu1_2\",\n",
    "            '8': \"relu2_2\",\n",
    "            '15': \"relu3_3\",\n",
    "            '22': \"relu4_3\"\n",
    "        }\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "        for name, module in self.vgg_layers._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layer_name_mapping:\n",
    "                output[self.layer_name_mapping[name]] = x\n",
    "        return LossOutput(**output)\n",
    "\n",
    "# loss_net=LossNetwork(models.vgg19(pretrained=True))\n",
    "\n",
    "def perp_loss(x,x_hat,p_loss):\n",
    "    b=p_loss(x)-p_loss(x_hat)\n",
    "    out=np.lilang.norm(b)\n",
    "    return out\n",
    "    \n",
    "# class GramMatrix(nn.Module):\n",
    "#     def forward(self, input):\n",
    "#         b, c, h, w = input.size()\n",
    "#         f = input.view(b, c, h * w) # b x c x(hxw)\n",
    "#         # torch.bmm(batch1, batch2, out=None)\n",
    "#         # batch1 : bxmxp, batch2 : bxpxn -> bxmxn\n",
    "#         G = torch.bmm(f, f.transpose(1, 2)) \n",
    "#         # f: bxcx(hxw), f.transpose: bx(hxw)xc -> bxcxc\n",
    "#         return G.div_(h * w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(2000, 28, 28)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(digits).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-16-c2906a6b8d4d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0ma\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mresize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdigits\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'resize' is not defined"
     ]
    }
   ],
   "source": [
    "a=resize(np.array(digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_resized=aa\n",
    "print(mnist_resized[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_loader=DataLoader(mnist_d,batch_size=BATCH_SIZE,shuffle=True)\n",
    "loss_l2 = torch.nn.MSELoss()\n",
    "loss_l1=torch.nn.L1Loss()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "def l2_l1_loss(true_img,out_img,l1,l2,net,ids):\n",
    "    norm_weights=torch.square(net.embedding_content(ids))\n",
    "    L1= (1/2)*l2(true_img,out_img)\n",
    "    L2=(1/2)*l1(true_img,out_img)\n",
    "    WDecay=WD*torch.sum(norm_weights)\n",
    "    return L1+L2 + WDecay\n",
    "\n",
    "col_content=[]\n",
    "for j in range(CONTENT_DIM):\n",
    "    col_content.append(\"x{}\".format(j))\n",
    "\n",
    "col_content.append(\"Epoch\")\n",
    "\n",
    "col_class=[]\n",
    "for j in range(CLASS_DIM):\n",
    "    col_class.append(\"x{}\".format(j))\n",
    "col_class.append(\"Epoch\")\n",
    "\n",
    "\n",
    "class_embeddings_time=pd.DataFrame(columns=col_class)\n",
    "content_embedding_time=pd.DataFrame(columns=col_content)\n",
    "\n",
    "j=0\n",
    "optimizer = torch.optim.Adam(cur_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    total_loss=0\n",
    "    i=0\n",
    "    cur_model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images,classes,ids=batch\n",
    "        output=cur_model.forward(classes,ids)\n",
    "        loss=l2_l1_loss(images.squeeze(),output.squeeze(),loss_l1,loss_l2,cur_model,ids)\n",
    "#         loss=perp_loss(images,resized_for_vgg,loss_net)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "        i+=1\n",
    "    print(\"not crash\")\n",
    "    if epoch%10==0:\n",
    "        conc_img = torch.cat((images[0],output[0][np.newaxis,:,:]),dim=2)\n",
    "        writer.add_image(\"True image vs generated  in epoch {}\".format(epoch)\n",
    "                         ,conc_img)\n",
    "        print(\"not crash\")\n",
    "        with torch.no_grad():\n",
    "            activation = {}\n",
    "            def get_activation(name):\n",
    "                def hook(model, input, output):\n",
    "                    activation[name] = output.detach()\n",
    "                return hook\n",
    "            print(\"not crash\")\n",
    "\n",
    "            model=cur_model\n",
    "            num_contnet=1\n",
    "            print(\"not crash\")\n",
    "            model.generator.net[10].register_forward_hook(get_activation('ext_conv1'))\n",
    "            print(\"not crash\")\n",
    "\n",
    "            x = torch.tensor([2,1])\n",
    "            y=torch.tensor([num_contnet,1])\n",
    "            output = model(x,y)\n",
    "            print(\"not crash\")\n",
    "\n",
    "            act = activation['ext_conv1'].squeeze()\n",
    "            print(\"not crash\")\n",
    "\n",
    "            num_plot = 4\n",
    "            fig, axarr = plt.subplots(4,4,figsize=(8,8))\n",
    "            \n",
    "            images=[]\n",
    "            fig.suptitle(\"Activation layers , generate digit 2 with thick 4 content vector , epoch {}\".format(epoch))\n",
    "            for i,ax  in enumerate(axarr.flatten()):\n",
    "                ax.imshow(act[0][i,:,:])\n",
    "            print(\"not crash\")\n",
    "\n",
    "            cur_model.eval()\n",
    "            print(\"not crash\")\n",
    "\n",
    "            pic=cur_model.forward(torch.tensor([2,1]),torch.tensor([1,1]))\n",
    "            print(\"not crash\")\n",
    "\n",
    "            fig=plt.figure()\n",
    "            fig.subplots_adjust(bottom = 0)\n",
    "            fig.subplots_adjust(top = 1)\n",
    "            fig.subplots_adjust(right = 1)\n",
    "            fig.subplots_adjust(left = 0)\n",
    "            a = torch.tensor(mnist_d.data[1][0]).squeeze()\n",
    "            b = torch.tensor(mnist_d.data[num_contnet][0]).squeeze()\n",
    "            c = pic[0]\n",
    "            d=torch.cat((a,b,c),dim=1)\n",
    "            print(\"not crash\")\n",
    "\n",
    "            fig = plt.figure(frameon=False, facecolor='white')\n",
    "            ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "            ax.set_axis_off()\n",
    "            fig.add_axes(ax)\n",
    "            ax.imshow(d)\n",
    "            plt.show()\n",
    "\n",
    "        cur_model.save_weights(\"weights_trained\\\\trained_mnist_gen_loss_{}\".format(str(total_loss/i)[2:6]))\n",
    "        print(\"loss in epoch {}\".format(epoch),total_loss/i)\n",
    "\n",
    "#     if epoch%5==0:\n",
    "#         class_embeddings=np.zeros((10,CLASS_DIM+1))\n",
    "#         content_embeddings=np.zeros((100,CONTENT_DIM+1))\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for i in range(10):\n",
    "#                 class_embeddings[i,:CLASS_DIM]=cur_model.embedding_class(torch.tensor(i))\n",
    "#             for i in range(100):\n",
    "#                 content_embeddings[i,:CONTENT_DIM]=cur_model.embedding_content(torch.tensor(i))\n",
    "\n",
    "\n",
    "        # class_embeddings[:,CLASS_DIM]=epoch\n",
    "        # content_embeddings[:,CONTENT_DIM]=epoch\n",
    "        # df = pd.DataFrame(class_embeddings,columns=col_class)\n",
    "        # class_emb_df=df\n",
    "        # content_emb_df_curr=pd.DataFrame(content_embeddings,columns=col_content)\n",
    "\n",
    "        # content_embedding_time=content_embedding_time.append(content_emb_df_curr)\n",
    "        # class_embeddings_time=class_embeddings_time.append(class_emb_df)\n",
    "        #\n",
    "        j+=1\n",
    "        # writer.add_embedding(class_embeddings, metadata=np.arange(10),tag=\"Class embeddings epoch\"+str(epoch),label_img=torch.tensor(num_pics))\n",
    "        # writer.add_embedding(content_embeddings, metadata=np.arange(100),tag=\"Content embeddings epoch {}\".format(epoch),label_img=torch.tensor(image[:100,:,:,:]))\n",
    "\n",
    "    # writer.add_scalar(\"loss\",total_loss/i,epoch)\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    class_embeddings=np.zeros((10,CLASS_DIM))\n",
    "    for i in range(10):\n",
    "        class_embeddings[i,:]=cur_model.get_emb_class(torch.tensor(i))\n",
    "\n",
    "    id_embeddings=np.zeros((2000,CONTENT_DIM))\n",
    "    for id in range(2000):\n",
    "        id_embeddings[id,:]=cur_model.get_emb_content(torch.tensor(id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "writer.add_embedding(class_embeddings, metadata=np.arange(10),tag=\"Class embeddings1\")\n",
    "writer.add_embedding(id_embeddings,metadata=np.arange(2000),tag=\"Content embedding1\",label_img=torch.tensor(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from  scipy.spatial import distance_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "curr_PCA=PCA(n_components=2)\n",
    "curr_embeddings=curr_PCA.fit_transform(class_embeddings)\n",
    "curr_data=pd.DataFrame(curr_embeddings,columns=[\"PC1\",\"PC2\"])\n",
    "curr_data[\"class_num\"]=np.arange(10)\n",
    "display(curr_data)\n",
    "fig=px.scatter(curr_data,hover_data=[\"class_num\"],x=\"PC1\",y=\"PC2\")\n",
    "fig.show()\n",
    "print(curr_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Does the class embeddings make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from  scipy.spatial import distance_matrix\n",
    "col_epochs=(np.full(5,10))*(np.arange(0,5))\n",
    "distance_transition=pd.DataFrame(columns=col_epochs)\n",
    "for i in col_epochs:\n",
    "    class_embeddings_curr=class_embeddings_time[class_embeddings_time[\"Epoch\"]==i]\n",
    "    dist_mat=distance_matrix(class_embeddings_curr,class_embeddings_curr)\n",
    "    dist_mat=np.where(dist_mat==0,np.inf,dist_mat)\n",
    "    distance_transition[i]=dist_mat.argmin(axis=1)\n",
    "\n",
    "distance_transition.insert(0,\"labels\",np.arange(10))\n",
    "plt.figure(figsize=((15,10)))\n",
    "display(pd.DataFrame(np.array(distance_transition[20]).reshape(1,-1)))\n",
    "pd.plotting.parallel_coordinates(distance_transition, 'labels',sort_labels=True,colormap=\"Paired\")\n",
    "plt.title(\"Class embedding nearest neighbor based on epochs with weight decay  {} \".format(WD))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Digits\")\n",
    "plt.yticks(np.arange(0,10))\n",
    "# fig = px.parallel_coordinates(distance_transition,color=\"labels\",title=\"Class embedding nearest neighbor based on epochs\")\n",
    "# fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "curr_PCA=PCA(n_components=3)\n",
    "curr_embeddings=curr_PCA.fit_transform(id_embeddings)\n",
    "curr_data=pd.DataFrame(curr_embeddings,columns=[\"PC1\",\"PC2\",\"PC3\"])\n",
    "curr_data[\"id\"]=np.arange(2000)\n",
    "display(curr_data)\n",
    "fig=px.scatter(curr_data,hover_data=[\"id\"],x=\"PC1\",y=\"PC2\",color=\"PC3\")\n",
    "fig.show()\n",
    "print(curr_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_model.eval()\n",
    "curr_pic=90\n",
    "curr_num=3\n",
    "with torch.no_grad():\n",
    "    pic=cur_model.forward(torch.tensor([curr_num,1]),torch.tensor([curr_pic,1]))\n",
    "    fig=plt.figure()\n",
    "    fig.subplots_adjust(bottom = 0)\n",
    "    fig.subplots_adjust(top = 1)\n",
    "    fig.subplots_adjust(right = 1)\n",
    "    fig.subplots_adjust(left = 0)\n",
    "    a = torch.tensor(mnist_d.data[curr_pic][0]).squeeze()\n",
    "    b = torch.tensor(mnist_d.data[curr_pic][0]).squeeze()\n",
    "    c = pic[0]\n",
    "    d=torch.cat((a,b,c),dim=1)\n",
    "\n",
    "    fig = plt.figure(frameon=False, facecolor='white')\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    ax.imshow(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model=cur_model\n",
    "model.generator.net[10].register_forward_hook(get_activation('ext_conv1'))\n",
    "x = torch.tensor([4,1])\n",
    "y=torch.tensor([20,1])\n",
    "output = model(x,y)\n",
    "zz=(torch.tensor(mnist_d.data[7][0]).squeeze())\n",
    "xx=torch.tensor(mnist_d.data[20][0]).squeeze()\n",
    "\n",
    "with torch.no_grad():\n",
    "    d=torch.cat((zz,xx,output[0]),dim=1)\n",
    "    plt.imshow(d)\n",
    "act = activation['ext_conv1'].squeeze()\n",
    "num_plot = 4\n",
    "print(act.shape)\n",
    "fig, axarr = plt.subplots(4,4,figsize=(8,8))\n",
    "images=[]\n",
    "print(axarr.shape)\n",
    "print(act.shape)\n",
    "fig.suptitle(\"Activation layers , generate digit 5 with rotated content vector\")\n",
    "for i,ax  in enumerate(axarr.flatten()):\n",
    "    ax.imshow(act[0][i,:,:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (ex5_r)",
   "language": "python",
   "name": "pycharm-445174ad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}